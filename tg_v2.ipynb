{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armughan/anaconda2/envs/py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/armughan/anaconda2/envs/py3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converts the data from chars to int \n",
    "def encode_data(datadir):\n",
    "    data=open(datadir,'r').read()#loading the file character by character in data\n",
    "    chars = list(set(data))#removing duplicate characters\n",
    "    vocab_size = len(chars)#counting unique characters\n",
    "    char_to_idx={ch:ind for ind,ch in enumerate(chars)} #making a dict for conv to int\n",
    "    idx_to_char={ind:ch for ind,ch in enumerate(chars)}#dict for converting int back to char (used in sampling)\n",
    "    encoded=np.array([char_to_idx[l] for l in data]) #converted data\n",
    "    return vocab_size,char_to_idx,idx_to_char,encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_batch(encoded,seq_per_batch,num_steps):\n",
    "    chars_per_batch=seq_per_batch*num_steps\n",
    "    num_batches=(encoded.size//chars_per_batch)\n",
    "#     num_batches-=int(encoded.size==num_batches*chars_per_batch)\n",
    "    encoded=encoded[:chars_per_batch*num_batches]\n",
    "    encoded=encoded.reshape((seq_per_batch,-1))\n",
    "    for i in range(num_batches-1):\n",
    "        x=encoded[:,i*num_steps:(i+1)*num_steps]\n",
    "        y=encoded[:,(i*num_steps)+1:((i+1)*num_steps)+1]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def form_placeholders(batch_size,num_steps):\n",
    "    x=tf.placeholder(tf.int32,shape=[batch_size,num_steps],name='inputs')\n",
    "    y=tf.placeholder(tf.int32,shape=[batch_size,num_steps],name='targets')\n",
    "    keep_prob=tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    return x,y,keep_prob\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def form_lstm_cell(num_of_layers,layer_size,keep_prob,batch_size):\n",
    "    lstm_cell=lambda :tf.contrib.rnn.BasicLSTMCell(layer_size)\n",
    "    dropout_cell=lambda :tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=keep_prob)\n",
    "    cell=tf.contrib.rnn.MultiRNNCell([dropout_cell() for i in range(num_of_layers)])\n",
    "    initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "    return cell,initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attach_fc_layers(lstm_output,lstmsize,num_neurons):\n",
    "    concat_outputs=tf.concat(lstm_output,axis=1)\n",
    "    flat_outputs=tf.reshape(concat_outputs,[-1,lstmsize])\n",
    "    logits=tf.layers.dense(flat_outputs,num_neurons)\n",
    "    preds=tf.nn.softmax(logits)\n",
    "    return preds,logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attach_loss(logits,y,vocab_size):\n",
    "    \n",
    "    y_one_hot=tf.one_hot(y,vocab_size)\n",
    "    y_reshaped=tf.reshape(y_one_hot,logits.get_shape())\n",
    "    entropies=tf.nn.softmax_cross_entropy_with_logits(labels=y_reshaped,logits=logits)\n",
    "    loss=tf.reduce_mean(entropies)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attach_optimizer(loss, learning_rate, grad_clip):\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    global_step=tf.Variable(0,trainable=False,name='global_step')\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars),global_step=global_step)\n",
    "    \n",
    "    return optimizer,global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class charlevelRNN:\n",
    "    def __init__(self,params):\n",
    "        sampling=params.get('sampling',False)\n",
    "        self.seq_per_batch=params.get('seq_per_batch',100)\n",
    "        self.num_steps=params.get('num_steps',50)\n",
    "        self.num_of_lstm_layers=params.get('num_of_lstm_layers',1)\n",
    "        self.lstm_layer_size=params.get('lstm_layer_size',500)\n",
    "        self.vocab_size=int(params['vocab_size'])\n",
    "        self.grad_clip=params.get('grad_clip',5)\n",
    "        self.lr=params.get('lr',0.001)\n",
    "        logdir=params.get('logdir',None)\n",
    "#        self.keep_prob=params.get('keep_prob',0.5)\n",
    "        if sampling:\n",
    "            self.seq_per_batch=1\n",
    "            self.num_steps=1\n",
    "            \n",
    "        #building new graph\n",
    "        tf.reset_default_graph()\n",
    "#         with tf.device('/gpu:0'):\n",
    "        #making placeholders\n",
    "        self.x_placeholder,self.y_placeholder,self.keep_prob_placeholder=form_placeholders(self.seq_per_batch,self.num_steps)\n",
    "        #forming multilayer lstm cells\n",
    "        cell,self.lstm_state=form_lstm_cell(self.num_of_lstm_layers,self.lstm_layer_size,self.keep_prob_placeholder,self.seq_per_batch)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.x_placeholder,self.vocab_size)\n",
    "\n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        lstm_outputs,self.new_lstm_state = tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=self.lstm_state)\n",
    "#         lstm_outputs,self.new_lstm_state = tf.nn.dynamic_rnn(cell,x_one_hot,dtype=tf.float32)\n",
    "        #running lstm outputs to the fc layers\n",
    "        self.preds,self.logits=attach_fc_layers(lstm_outputs,self.lstm_layer_size,self.vocab_size)\n",
    "\n",
    "\n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  attach_loss(self.logits,self.y_placeholder,self.vocab_size)\n",
    "        self.optimizer,self.step = attach_optimizer(self.loss,self.lr,self.grad_clip)\n",
    "\n",
    "        loss_summary=tf.summary.scalar('loss',self.loss)\n",
    "#         self.acc_summary=tf.summary.scalar('accuracy',self.accuracy)\n",
    "        if not sampling:\n",
    "            self.summaries=tf.summary.merge_all()\n",
    "            self.file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "        self.saver=tf.train.Saver()\n",
    "        self.initializer=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=50\n",
    "keep_prob=0.5\n",
    "seq_per_batch=100\n",
    "num_steps=150\n",
    "data_dir='shakespeare_input.txt'\n",
    "# data_dir='abc.txt'\n",
    "vocab_size,char_to_idx,idx_to_char,encoded=encode_data(data_dir)\n",
    "# save_every_n_iter=50\n",
    "log_every_n_iter=10\n",
    "# savedir='/output/my_rnn_model.ckpt'\n",
    "# term_log_file='/output/term_log.txt'\n",
    "savedir='/my_rnn_model.ckpt'\n",
    "term_log_file='/term_log.txt'\n",
    "\n",
    "model_params={\n",
    "    'sampling':False,\n",
    "    'seq_per_batch':seq_per_batch,\n",
    "    'num_steps':num_steps,\n",
    "    'num_of_lstm_layers':3,\n",
    "    'lstm_layer_size':550,\n",
    "    'vocab_size':vocab_size,\n",
    "#     'logdir':'/output/tf_logs_rnn/run/',\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'lr':0.01\n",
    "\n",
    "}\n",
    "term_log_file=open(term_log_file,'w')\n",
    "\n",
    "model=charlevelRNN(model_params)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(savedir):\n",
    "        model.saver.restore(sess,savedir)\n",
    "    else:\n",
    "        sess.run(model.initializer)\n",
    "    lstm_state=sess.run(model.lstm_state)\n",
    "    for epoch in range(num_epochs):\n",
    "        if(epoch%10==0):\n",
    "            model.lr/=10\n",
    "        print ('----------------------new epoch-----------------------------')\n",
    "        for x,y in get_data_batch(encoded,seq_per_batch,num_steps):\n",
    "            print ('epoch no=>',epoch)\n",
    "            step=sess.run(model.step)\n",
    "            print (\"Step no=> \",step)\n",
    "            feed_dict={model.x_placeholder:x,model.y_placeholder:y,model.lstm_state:lstm_state,model.keep_prob_placeholder:keep_prob}\n",
    "            if(step%log_every_n_iter==0):\n",
    "                _,loss,lstm_state,summary=sess.run([model.optimizer,model.loss,model.new_lstm_state,model.summaries],feed_dict=feed_dict)\n",
    "                model.file_writer.add_summary(summary,step)\n",
    "                print ('loss encountered ',loss)\n",
    "                term_log_file.write('epochno/step '+str(epoch)+'/'+str(step)+'=> loss encountered '+str(loss)+'\\n')\n",
    "                f=open('/output/ep_step.txt','a+')\n",
    "                f.write(str(epoch)+'/'+str(step)+'\\n')\n",
    "                f.close()\n",
    "            else:\n",
    "                _,loss,lstm_state=sess.run([model.optimizer,model.loss,model.new_lstm_state],feed_dict=feed_dict)\n",
    "        model.saver.save(sess,savedir)\n",
    "    model.file_writer.close()\n",
    "    term_log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4573338\n",
      "INFO:tensorflow:Restoring parameters from my_rnn_model.ckpt\n",
      "The horse was HFbAE.wlATH&lAF3lHtclPLlPALvlHtclPLlPALpgclPbHalbATT.:AHblbATlPT&\n",
      "FtYlbLlAT&lPAHgglITlbATl HBTx..,]rWwj]flVXf;oqM.:ATlbAFtYlFtlbATlPb&HtYT&PElHtclPL3TbF3TPlbATlPbH&PE.]PlbALplPAHgblITlbLlPTTlbAT3lHtclPL3TbF3TP.:AHblbATlPb&HtYTlbLlbAFPlPLtlL lbATlvH&P.wlAH\n",
      "TlPTtblbLLlPbHtcPlHlPLtElHtclbATlPbHbTE.]tclbATlbAFtYlbATlb&HFbF&PlL lbATlvL&gc.VFgglbATtlbAHblATlAHbAlbLLlPb&HtYTgalITTtE.]tclbAHblAFPlPbHbTlL lbATlvHalbATlPb&HtYTlPb&HtYT&.:AHblbLlATH\n",
      "TtlbATlPLtlbLlPTTlAF3lbLlbAFPE.:ATlbAFtYlbATlPT&\n",
      "FBTlL lbATlvL&gclPAHgglIT.]tclbATlb&H\n",
      "TggT&lL lbATlvL&gclHPlbATa.:AHblbLlAT&lbLlbATlbAFtYlHtclATH\n",
      "TtlPAHgglPb&HFYAb.:ATlbAFT lbATalATH&lbATlPb&TtYbAlL lbAFPlbAFtYPElbAHb.wlAH\n",
      "TlITTtlbLlbHOTlbATlbALpPHtclbAFtYE.]tclbAHblbATlPT&\n",
      "FBTlL lAFPlATHcPlbAHblPALvPE.:AHblbATlbAFtYlbALplAHPblPL3TbAFtYlbATlITH&PlHtclbATlPb&HtYT&E.:ATlPbHbpTlbLlATH&lbATlPbH&PlL lAF3lbAT&T.wtlb&THPLtlbLlbATlbA&LtTElHPlHl3L&TlPb&HFYAb.:LlPTTlbAT3lbATtlHtclPb&LtYT&ElHtclbAFPlPbH&P.:LlPTTlAF3ElbAHblAH\n",
      "TlbLlbATTlHtclPL3TlPL&&Lv.:LlbATlPbH&\n"
     ]
    }
   ],
   "source": [
    "#forming samples\n",
    "seed='The horse was '\n",
    "\n",
    "num_samples=1000\n",
    "keep_prob=1\n",
    "data_dir='shakespeare_input.txt'\n",
    "vocab_size,char_to_idx,idx_to_char,encoded=encode_data(data_dir)\n",
    "print (encoded.size)\n",
    "sequence=[char_to_idx[seedchar] for seedchar in seed]\n",
    "# savedir='/output/my_rnn_model.ckpt'\n",
    "savedir='my_rnn_model.ckpt'\n",
    "model_params={\n",
    "    'sampling':True,\n",
    "    'num_of_lstm_layers':3,\n",
    "    'lstm_layer_size':550,\n",
    "    'vocab_size':vocab_size \n",
    "}\n",
    "\n",
    "\n",
    "model=charlevelRNN(model_params)\n",
    "\n",
    "\n",
    "def select_from_probs(probs,top_n,vocab_size):\n",
    "    probs=probs.flatten()\n",
    "    probs[np.argsort(probs)[:-top_n]]=0\n",
    "    probs=probs/sum(probs)\n",
    "    return np.random.choice(vocab_size,1,p=probs)\n",
    "\n",
    "\n",
    "model=charlevelRNN(model_params)\n",
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess,savedir)\n",
    "    lstm_state=sess.run(model.lstm_state)\n",
    "    for ci in sequence:#seeding the model\n",
    "        x=np.array([[ci]])\n",
    "        feed_dict={model.x_placeholder:x,model.lstm_state:lstm_state,model.keep_prob_placeholder:keep_prob}\n",
    "        lstm_state=sess.run(model.new_lstm_state,feed_dict=feed_dict)\n",
    "        \n",
    "    for i in range(num_samples):\n",
    "        x=np.array([[sequence[-1]]])\n",
    "        feed_dict={model.x_placeholder:x,model.lstm_state:lstm_state,model.keep_prob_placeholder:keep_prob}\n",
    "        probs,lstm_state=sess.run([model.preds,model.new_lstm_state],feed_dict=feed_dict)\n",
    "        si=select_from_probs(probs,2,vocab_size)\n",
    "        sequence.append(si[0])\n",
    "\n",
    "conv_list=[idx_to_char[ind] for ind in sequence ]\n",
    "print (''.join(conv_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
